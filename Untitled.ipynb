{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c098c81-be47-4214-9fb2-8c536ae2c2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45ac04c61594ac6a42e572e54459b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6891652-a90f-4304-8a54-94ad5394c868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS 사용 가능. 장치: mps, 모델 dtype: torch.float16\n",
      "모델을 장치 mps:0로 이동 완료. 현재 모델 dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM # BitsAndBytesConfig 임포트 제거\n",
    "\n",
    "# 0. 모델 ID 정의\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# 1. 장치 설정 및 모델 로드 시 사용할 dtype 결정\n",
    "#    양자화를 사용하지 않으므로, 모델의 부동소수점 정밀도를 직접 설정합니다.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    # CUDA에서는 bfloat16이 Gemma 모델에 권장될 수 있으며, float16도 사용 가능합니다.\n",
    "    # Ampere 아키텍처 이상 GPU는 bfloat16을 잘 지원합니다.\n",
    "    model_dtype = torch.bfloat16\n",
    "    print(f\"CUDA 사용 가능. 장치: {device}, 모델 dtype: {model_dtype}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    # MPS는 float16을 잘 지원합니다.\n",
    "    model_dtype = torch.float16\n",
    "    print(f\"MPS 사용 가능. 장치: {device}, 모델 dtype: {model_dtype}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    # CPU에서는 bfloat16 (최신 CPU 및 PyTorch 버전) 또는 float32를 사용합니다.\n",
    "    # 안정성을 위해 float32를 기본으로 하되, bfloat16도 시도해볼 수 있습니다.\n",
    "    try:\n",
    "        # CPU에서 bfloat16 지원 테스트 (간단한 텐서 생성)\n",
    "        _ = torch.randn(1, dtype=torch.bfloat16)\n",
    "        model_dtype = torch.bfloat16\n",
    "        print(\"CPU에서 bfloat16 지원 확인됨.\")\n",
    "    except RuntimeError:\n",
    "        model_dtype = torch.float32\n",
    "        print(\"CPU에서 bfloat16 미지원 또는 오류 발생. float32 사용.\")\n",
    "    print(f\"CPU 사용. 장치: {device}, 모델 dtype: {model_dtype}\")\n",
    "\n",
    "\n",
    "# 2. 모델 로드 (양자화 설정 제거, torch_dtype 명시)\n",
    "try:\n",
    "    model = Gemma3ForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=model_dtype, # 결정된 dtype으로 모델 로드\n",
    "        # device_map=\"auto\" # 메모리가 매우 부족할 경우 고려해볼 수 있으나,\n",
    "                           # 단일 장치 지정 시에는 보통 .to(device)를 사용합니다.\n",
    "    ).eval() # 추론 모드\n",
    "except Exception as e:\n",
    "    print(f\"모델 로딩 중 오류 발생: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 3. 토크나이저 로드\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        print(f\"tokenizer.pad_token_id가 없어 eos_token_id({tokenizer.eos_token_id})로 설정합니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"토크나이저 로딩 중 오류 발생: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 4. 모델을 최종 장치로 이동\n",
    "#    from_pretrained에서 device_map을 사용하지 않았으므로 .to(device) 호출\n",
    "model.to(device)\n",
    "print(f\"모델을 장치 {str(model.device)}로 이동 완료. 현재 모델 dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f391edd1-6d0d-4b25-a40d-67223e1a7378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Gemma-3 번역 테스트 (양자화 없음) 시작 ---\n",
      "\n",
      "[원본 (English)]\n",
      "Hi are you good at translating?\n",
      "  프롬프트 (모델 입력): <bos><start_of_turn>user\n",
      "You are a translation assistant that translates user input into Korean. Provide only the Korean translation.\n",
      "\n",
      "Please translate the following text into Korean. Do not add any explanations or introductory phrases. Just provide the Korean translation.\n",
      "\n",
      "Text to translate: \"Hi ar...\n",
      "  사용할 EOS 토큰 ID(들): [1, 106]\n",
      "  모델 생성 부분 (special tokens 스킵 후 첫 150자): 안녕하세요, 번역 잘 하세요?...\n",
      "[번역 결과 (Korean)]\n",
      "안녕하세요, 번역 잘 하세요?\n",
      "------------------------------\n",
      "\n",
      "[원본 (Japanese)]\n",
      "こんにちは。あなたは翻訳が上手ですか？\n",
      "  프롬프트 (모델 입력): <bos><start_of_turn>user\n",
      "You are a translation assistant that translates user input into Korean. Provide only the Korean translation.\n",
      "\n",
      "Please translate the following text into Korean. Do not add any explanations or introductory phrases. Just provide the Korean translation.\n",
      "\n",
      "Text to translate: \"こんにちは...\n",
      "  사용할 EOS 토큰 ID(들): [1, 106]\n",
      "  모델 생성 부분 (special tokens 스킵 후 첫 150자): 안녕하세요. 당신은 번역이 잘 하시나요?...\n",
      "[번역 결과 (Korean)]\n",
      "안녕하세요. 당신은 번역이 잘 하시나요?\n",
      "------------------------------\n",
      "\n",
      "[원본 (Chinese_simplified)]\n",
      "你好，你翻译得好吗？\n",
      "  프롬프트 (모델 입력): <bos><start_of_turn>user\n",
      "You are a translation assistant that translates user input into Korean. Provide only the Korean translation.\n",
      "\n",
      "Please translate the following text into Korean. Do not add any explanations or introductory phrases. Just provide the Korean translation.\n",
      "\n",
      "Text to translate: \"你好，你翻...\n",
      "  사용할 EOS 토큰 ID(들): [1, 106]\n",
      "  모델 생성 부분 (special tokens 스킵 후 첫 150자): 안녕하세요, 번역 잘 되셨어요?...\n",
      "[번역 결과 (Korean)]\n",
      "안녕하세요, 번역 잘 되셨어요?\n",
      "------------------------------\n",
      "\n",
      "[원본 (Spanish)]\n",
      "Hola. ¿Eres bueno traduciendo?\n",
      "  프롬프트 (모델 입력): <bos><start_of_turn>user\n",
      "You are a translation assistant that translates user input into Korean. Provide only the Korean translation.\n",
      "\n",
      "Please translate the following text into Korean. Do not add any explanations or introductory phrases. Just provide the Korean translation.\n",
      "\n",
      "Text to translate: \"Hola....\n",
      "  사용할 EOS 토큰 ID(들): [1, 106]\n",
      "  모델 생성 부분 (special tokens 스킵 후 첫 150자): 안녕하세요. 번역 잘 하세요?...\n",
      "[번역 결과 (Korean)]\n",
      "안녕하세요. 번역 잘 하세요?\n",
      "------------------------------\n",
      "\n",
      "[원본 (Korean_input)]\n",
      "안녕 너는 번역을 잘하니?\n",
      "  프롬프트 (모델 입력): <bos><start_of_turn>user\n",
      "You are a translation assistant that translates user input into Korean. Provide only the Korean translation.\n",
      "\n",
      "Please translate the following text into Korean. Do not add any explanations or introductory phrases. Just provide the Korean translation.\n",
      "\n",
      "Text to translate: \"안녕 너는...\n",
      "  사용할 EOS 토큰 ID(들): [1, 106]\n",
      "  모델 생성 부분 (special tokens 스킵 후 첫 150자): 안녕하세요, 당신은 번역을 잘 하시나요?...\n",
      "[번역 결과 (Korean)]\n",
      "안녕하세요, 당신은 번역을 잘 하시나요?\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 5. 번역 함수 정의\n",
    "def translate_to_korean_gemma3(text_to_translate: str,\n",
    "                               model: Gemma3ForCausalLM,\n",
    "                               tokenizer: AutoTokenizer,\n",
    "                               max_new_tokens: int = 512,\n",
    "                               temperature: float = 0.05,\n",
    "                               top_p: float = 0.9):\n",
    "    messages = [\n",
    "        # 시스템 메시지는 모델의 전반적인 역할(번역가)을 설정하는 데 도움을 줄 수 있습니다.\n",
    "        # 하지만 때로는 시스템 메시지 없이 user 메시지만으로도 충분할 수 있습니다.\n",
    "        # 우선은 시스템 메시지를 간결하게 유지하거나, user 메시지에 모든 것을 포함하는 것을 테스트해봅니다.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a translation assistant that translates user input into Korean. Provide only the Korean translation.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            # 사용자의 요청임을 명확히 하고, 번역할 텍스트를 명시적으로 전달합니다.\n",
    "            \"content\": f\"Please translate the following text into Korean. Do not add any explanations or introductory phrases. Just provide the Korean translation.\\n\\nText to translate: \\\"{text_to_translate}\\\"\"\n",
    "        }\n",
    "        # add_generation_prompt=True 옵션으로 인해 이 뒤에 모델이 응답을 시작할 부분(예: <start_of_turn>model\\n)이 추가됩니다.\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        prompt_text_for_model = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"tokenizer.apply_chat_template 사용 중 오류 또는 미지원: {e}. 대체 프롬프트를 사용합니다.\")\n",
    "        # 대체 프롬프트는 Gemma의 특정 형식을 따르는 것이 좋습니다.\n",
    "        # 예: prompt_text_for_model = f\"<start_of_turn>system\\nYou are a highly skilled translator...\\n<end_of_turn>\\n<start_of_turn>user\\n{text_to_translate}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "        # 여기서는 apply_chat_template이 성공했다고 가정하고 단순화합니다.\n",
    "        # 실제로는 Gemma 모델 카드에서 권장하는 정확한 수동 프롬프트 형식을 사용해야 합니다.\n",
    "        prompt_text_for_model = f\"System: You are a highly skilled translator... Stop generating text immediately after the Korean translation is complete.\\nUser: {text_to_translate}\\nAssistant:\"\n",
    "\n",
    "    print(f\"  프롬프트 (모델 입력): {prompt_text_for_model[:300]}...\")\n",
    "    inputs = tokenizer(prompt_text_for_model, return_tensors=\"pt\", truncation=True, padding=\"longest\", return_attention_mask=True).to(model.device)\n",
    "\n",
    "    try:\n",
    "        with torch.inference_mode():\n",
    "            eos_token_ids_list = [tokenizer.eos_token_id]\n",
    "            try:\n",
    "                eot_id = tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "                if isinstance(eot_id, int) and eot_id != tokenizer.unk_token_id: # 유효한 ID이고 UNK 토큰이 아니면\n",
    "                    if eot_id not in eos_token_ids_list: # 중복 방지\n",
    "                         eos_token_ids_list.append(eot_id)\n",
    "                print(f\"  사용할 EOS 토큰 ID(들): {eos_token_ids_list}\")\n",
    "            except Exception as e_eot:\n",
    "                 print(f\"  <end_of_turn> 토큰 ID 확인 중 오류 (무시하고 기본 EOS 사용): {e_eot}\")\n",
    "\n",
    "\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                eos_token_id=eos_token_ids_list, # 단일 ID 또는 ID 리스트\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "            )\n",
    "\n",
    "        output_tokens = outputs[0]\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        \n",
    "        translated_text = \"\"\n",
    "        if output_tokens.shape[0] > input_length:\n",
    "            generated_tokens = output_tokens[input_length:]\n",
    "            # special_tokens=False로 먼저 디코딩하여 EOS 문자열을 찾기 위함이 아닙니다.\n",
    "            # EOS를 포함하여 디코딩 후 자르는 것이 더 정확합니다.\n",
    "            # 하지만, generate 함수가 eos_token_id를 만나면 그 이후는 생성하지 않아야 합니다.\n",
    "            # 그럼에도 불구하고 추가 생성이 있다면, 디코딩 후 처리해야 합니다.\n",
    "            \n",
    "            # skip_special_tokens=True로 디코딩 (일반적인 경우)\n",
    "            translated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "            print(f\"  모델 생성 부분 (special tokens 스킵 후 첫 150자): {translated_text[:150]}...\")\n",
    "\n",
    "            # 만약 skip_special_tokens=True로도 이상한 것이 붙는다면,\n",
    "            # skip_special_tokens=False로 디코딩하여 실제 EOS 토큰 문자열을 기준으로 잘라내야 합니다.\n",
    "            # 이 부분은 실험을 통해 어떤 방식이 더 나은지 결정해야 합니다.\n",
    "            # 예시:\n",
    "            # raw_decoded_text = tokenizer.decode(generated_tokens, skip_special_tokens=False).strip()\n",
    "            # eos_str_candidates = [tokenizer.eos_token]\n",
    "            # if \"<end_of_turn>\" in tokenizer.vocab and tokenizer.convert_tokens_to_ids(\"<end_of_turn>\") != tokenizer.unk_token_id:\n",
    "            #     eos_str_candidates.append(\"<end_of_turn>\")\n",
    "            #\n",
    "            # for eos_candidate in eos_str_candidates:\n",
    "            #     if eos_candidate in raw_decoded_text:\n",
    "            #         translated_text = raw_decoded_text.split(eos_candidate)[0].strip()\n",
    "            #         break # 첫 번째 발견된 EOS에서 자름\n",
    "            # else: # EOS가 명시적으로 발견되지 않으면, 일단 skip_special_tokens=True 결과 사용\n",
    "            #     translated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        else: # 생성이 없거나 입력과 길이가 같은 경우\n",
    "            translated_text = \"\"\n",
    "\n",
    "\n",
    "        # 추가적인 휴리스틱 정제 (임시방편, 최후의 수단)\n",
    "        # 알려진 이상한 외국어 패턴의 시작 부분을 기준으로 자르기\n",
    "        # 이 패턴들은 실제 결과에 따라 계속 추가/수정해야 합니다.\n",
    "        artifacts_to_check = [\n",
    "            \"Ьaino\", \"יו\", \"tained\", \" कदाचित\", \"Ь অনুসরণ করেছেন\", \"سلم\", \"pengguna\",\n",
    "            \"Climbs to the top\", \"Note:\", \"Please ensure\", \"Okay, let's get started\",\n",
    "            \"Alright, let me see\", \"Translation note:\", \"Also, the translation uses\"\n",
    "        ]\n",
    "        # 정규 표현식을 사용하여 한국어 블록 이후의 외국어를 제거하는 것이 더 나을 수 있습니다.\n",
    "        # 예: import re; translated_text = re.split(r'[^\\uAC00-\\uD7A3\\u3131-\\u3163\\u1100-\\u11FFa-zA-Z0-9\\s.,!?\\'\"]+', translated_text)[0]\n",
    "        \n",
    "        original_length = len(translated_text)\n",
    "        for artifact in artifacts_to_check:\n",
    "            if artifact.lower() in translated_text.lower(): # 대소문자 무시하고 체크\n",
    "                # translated_text = translated_text.split(artifact)[0].strip() # 대소문자 구분하여 자름\n",
    "                # 대소문자 무시하고 자르려면 정규식 필요\n",
    "                import re\n",
    "                # 패턴을 찾아서 그 이전까지만 남김\n",
    "                match = re.search(re.escape(artifact), translated_text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    translated_text = translated_text[:match.start()].strip()\n",
    "        \n",
    "        if len(translated_text) < original_length:\n",
    "            print(f\"  후처리로 일부 아티팩트 제거됨. (원래 길이: {original_length}, 수정 후: {len(translated_text)})\")\n",
    "\n",
    "\n",
    "        # 번역 결과가 비어있거나 너무 짧으면 원본 텍스트와 비교하여 로깅\n",
    "        if not translated_text or len(translated_text) < 5:\n",
    "            print(f\"⚠️ 번역 결과가 매우 짧거나 비어있음: '{translated_text}'. 원본: '{text_to_translate}'\")\n",
    "\n",
    "\n",
    "        return translated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"번역 생성 중 오류 발생: {e}\")\n",
    "        return f\"번역 오류: {e}\"\n",
    "\n",
    "# 6. 다양한 언어 텍스트 예시 및 번역 실행 (이전과 동일)\n",
    "texts_to_translate = {\n",
    "    \"english\": \"Hi are you good at translating?\",\n",
    "    \"japanese\": \"こんにちは。あなたは翻訳が上手ですか？\",\n",
    "    \"chinese_simplified\": \"你好，你翻译得好吗？\",\n",
    "    \"spanish\": \"Hola. ¿Eres bueno traduciendo?\",\n",
    "    \"korean_input\": \"안녕 너는 번역을 잘하니?\"\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'model' not in locals() or 'tokenizer' not in locals():\n",
    "        print(\"모델 또는 토크나이저가 성공적으로 로드되지 않았습니다.\")\n",
    "    else:\n",
    "        print(f\"\\n--- Gemma-3 번역 테스트 (양자화 없음) 시작 ---\")\n",
    "        for lang, text in texts_to_translate.items():\n",
    "            print(f\"\\n[원본 ({lang.capitalize()})]\")\n",
    "            print(text)\n",
    "\n",
    "            translated = translate_to_korean_gemma3(text, model, tokenizer)\n",
    "\n",
    "            print(f\"[번역 결과 (Korean)]\")\n",
    "            print(translated)\n",
    "            print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43d79a-21f5-4f8c-ab59-8042d2fa4459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee28bd04-f4d2-4a55-a0f6-e690ff4cd414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4dc518-4833-4684-9b25-fda1d2860059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stt_project)",
   "language": "python",
   "name": "stt_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
