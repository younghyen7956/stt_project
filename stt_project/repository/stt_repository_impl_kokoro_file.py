import base64

import pandas as pd
from pydub import AudioSegment
from dotenv import load_dotenv
from fastapi import UploadFile
from starlette.concurrency import run_in_threadpool
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, AutoModel
from transformers import pipeline
import soundfile as sf
import numpy as np
import io, ssl, whisper, torch, openai, json, os, pycountry, random
from typing import Optional, Dict, Any

# Kokoro TTS ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    from kokoro import KPipeline
except ImportError:
    print("WARNING: Kokoro library not found. Please install it to use Kokoro TTS.")
    KPipeline = None

ssl._create_default_https_context = ssl._unverified_context
from stt_project.repository.stt_repository import SttRepository

# --- Kokoro TTS ì§€ì› ì–¸ì–´ ë° í”„ë¦¬ì…‹ ì •ì˜ ---
# Kokoroê°€ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'a', 'b', 'e', 'f', 'h', 'i', 'j', 'p', 'z')
KOKORO_SUPPORTED_LANGS = ['a', 'b', 'e', 'f', 'h', 'i', 'j', 'p', 'z']

# ì–¸ì–´ ì½”ë“œë³„ ê¸°ë³¸ ìŒì„± í”„ë¦¬ì…‹ ë§¤í•‘
# ì œê³µí•´ì£¼ì‹  í”„ë¦¬ì…‹ ëª©ë¡ì„ ë°”íƒ•ìœ¼ë¡œ, ê° ì–¸ì–´ ì½”ë“œì— ë§ëŠ” ëŒ€í‘œ í”„ë¦¬ì…‹ì„ ì„ ì •í–ˆìŠµë‹ˆë‹¤.
# í•„ìš”ì— ë”°ë¼ ë” ë‹¤ì–‘í•˜ê²Œ ë§¤í•‘í•˜ê±°ë‚˜, ëœë¤ ì„ íƒ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
KOKORO_VOICE_PRESETS = {
    'a': ['am_adam', 'am_echo', 'am_eric', 'am_fenrir', 'am_liam', 'am_michael', 'am_onyx', 'am_puck', 'am_santa'],
    # American English
    'b': ['bf_alice', 'bf_emma', 'bf_isabella', 'bf_lily', 'bm_daniel', 'bm_fable', 'bm_george', 'bm_lewis'],
    # British English
    'e': ['ef_dora', 'em_alex', 'em_santa'],  # Spanish (es)
    'f': ['ff_siwis'],  # French (fr-fr)
    'h': ['hf_alpha', 'hf_beta', 'hm_omega', 'hm_psi'],  # Hindi (hi)
    'i': ['if_sara', 'im_nicola'],  # Italian (it)
    'j': ['jf_alpha', 'jf_gongitsune', 'jf_nezumi', 'jf_tebukuro', 'jm_kumo'],  # Japanese (ja)
    'p': ['pf_dora', 'pm_alex', 'pm_santa'],  # Brazilian Portuguese (pt-br)
    'z': ['zf_xiaobei', 'zf_xiaoni', 'zf_xiaoxiao', 'zf_xiaoyi', 'zm_yunjian', 'zm_yunxi', 'zm_yunxia', 'zm_yunyang']
    # Mandarin Chinese (zh)
}


# --- // Kokoro TTS ì§€ì› ì–¸ì–´ ë° í”„ë¦¬ì…‹ ì •ì˜ ---


class SttRepositoryImpl(SttRepository):
    __instance = None
    load_dotenv()

    def __new__(cls, *args, **kwargs):
        if cls.__instance is None:
            cls.__instance = super().__new__(cls)
        return cls.__instance

    @classmethod
    def getInstance(cls):
        if cls.__instance is None:
            cls.__instance = cls()
        return cls.__instance

    def __init__(self):
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            self.torch_dtype = torch.bfloat16
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            self.torch_dtype = torch.bfloat16
        else:
            self.device = torch.device("cpu")
            self.torch_dtype = torch.float16

        self.pipelines = {}
        self.gpt_model = None
        self.whisper_detection_model = None
        self.bark_processor = None
        self.bark_model = None
        self.kokoro_tts_pipeline = None

        self.get_model()
        print("--- SttRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì™„ë£Œ ---")

    def get_model(self):
        model_names_str = os.getenv("STTMODELS", "openai/whisper-large-v3-turbo")
        model_names = [name.strip() for name in model_names_str.split(',')]

        for model_name in model_names:
            print(f"\n--- Whisper ëª¨ë¸ ({model_name}) ë¡œë“œ ì‹œì‘ ---")
            try:
                processor = AutoProcessor.from_pretrained(model_name)
                model = AutoModelForSpeechSeq2Seq.from_pretrained(
                    model_name, torch_dtype=self.torch_dtype, low_cpu_mem_usage=(self.device.type == "cpu"),
                    use_safetensors=True
                ).to(self.device)

                self.pipelines[model_name] = pipeline(
                    "automatic-speech-recognition", model=model, tokenizer=processor.tokenizer,
                    feature_extractor=processor.feature_extractor, torch_dtype=self.torch_dtype,
                    device=self.device, return_timestamps=True,
                )
                print(f"âœ… Whisper ëª¨ë¸ ({model_name}) ë° ASR íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ.")
            except Exception as e:
                print(f"âŒ Whisper ëª¨ë¸ ({model_name}) ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

        openai_api_key = os.getenv("OPENAI_API_KEY")
        if openai_api_key:
            self.gpt_model = openai.OpenAI(api_key=openai_api_key)
            print("âœ… OpenAI GPT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")
        else:
            print("âš ï¸ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ì–´ OpenAI GPT í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.gpt_model = None

        print("\n--- (ì–¸ì–´ ê°ì§€ìš©) Whisper ëª¨ë¸ ë¡œë“œ ì‹œì‘ ---")
        try:
            self.whisper_detection_model = whisper.load_model("base", device="cpu")
            print("âœ… (ì–¸ì–´ ê°ì§€ìš©) Whisper ëª¨ë¸ (base) ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            print(f"âŒ (ì–¸ì–´ ê°ì§€ìš©) Whisper ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

        # Bark TTS ëª¨ë¸ ë¡œë“œ ë¶€ë¶„ ì œê±°
        self.bark_processor = None
        self.bark_model = None

        print("\n--- Kokoro TTS ëª¨ë¸ ë¡œë“œ ì‹œì‘ ---")
        if KPipeline:
            try:
                # Kokoro KPipeline ì´ˆê¸°í™”. ê°€ì¥ ë³´í¸ì ì¸ 'a' (American English)ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì„¤ì •.
                self.kokoro_tts_pipeline = KPipeline(lang_code='a')
                print("âœ… Kokoro TTS KPipeline ë¡œë“œ ì™„ë£Œ.")
            except Exception as e:
                print(f"âŒ Kokoro TTS ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
                self.kokoro_tts_pipeline = None
        else:
            print("âš ï¸ Kokoro ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ Kokoro TTS ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        print("--- SttRepositoryImpl: get_model successfully ---")

    def universal_translation_sync(self, text_to_translate: str, source_lang: str, target_lang: str) -> str:
        if not self.gpt_model: return "GPT ë²ˆì—­ ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€"
        if not text_to_translate or not text_to_translate.strip(): return ""

        system_content = f"""You are a professional translator.
        Your task is to accurately translate the given text from {source_lang} to {target_lang}.
        Preserve the original formatting and meaning.
        Provide only the translated text, without any introductory phrases."""

        user_prompt = f"Translate the following text from {source_lang} to {target_lang}:\n\n{text_to_translate}"

        print(f"  (GPT ë²ˆì—­ ìš”ì²­: {source_lang} -> {target_lang} using gpt-4o-mini)")

        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_prompt}
        ]
        try:
            completion = self.gpt_model.chat.completions.create(model="gpt-4o-mini", messages=messages, temperature=0.0,
                                                                max_tokens=1024)
            return completion.choices[0].message.content.strip()
        except Exception as e:
            print(f"âŒ OpenAI GPT ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"GPT ë²ˆì—­ ì˜¤ë¥˜: {e}"

    def generate_llm_response_sync(self, text_input: str, original_language_code: str) -> dict:
        default_answers = {"answer_original_language": "ë‹µë³€ ìƒì„± ì‹¤íŒ¨", "answer_korean": "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"}
        if not self.gpt_model:
            default_answers["answer_original_language"] = "OpenAI API ì‚¬ìš© ë¶ˆê°€"
            default_answers["answer_korean"] = "OpenAI API ì‚¬ìš© ë¶ˆê°€"
            return default_answers
        if not text_input or text_input.strip() == "":
            default_answers["answer_original_language"] = "ì…ë ¥ í…ìŠ¤íŠ¸ ì—†ìŒ"
            default_answers["answer_korean"] = "ì…ë ¥ í…ìŠ¤íŠ¸ ì—†ìŒ"
            return default_answers

        original_lang_name_en = original_language_code
        try:
            # pycountryë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¸ì–´ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°. ì˜ˆ: 'ko' -> 'Korean'
            lang_obj = pycountry.languages.get(alpha_2=original_language_code.split('-')[0].lower())
            if lang_obj: original_lang_name_en = lang_obj.name
        except:
            pass

        system_prompt = f"""You are an AI expert specializing in South Korean public transportation.
        Your primary role is to provide the most optimal travel route from a starting point to a destination within South Korea, in {original_lang_name_en}.

        Always follow these steps to structure your response:
        1.  Propose one "Recommended Route" that is the most efficient, considering a balance of travel time, number of transfers, and cost.
        2.  Describe each step of the Recommended Route in detail (e.g., walking, subway line, bus number, direction, travel time, getting off station, exit number).
        3.  You can also suggest one or two "Alternative Routes" that might be worth considering.
        4.  Conclude with a summary of the "Estimated Total Time" and "Estimated Fare".
        5.  Provide only the helpful answer, without any introductory phrases like "Of course, here is the route...".
        """

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": "How do I get from Gyeongbokgung Palace to the COEX Aquarium by subway?"},
            {"role": "assistant", "content": f"""Here is the most efficient subway route from Gyeongbokgung Palace to the COEX Aquarium.

        **Recommended Route: Line 3 â†’ Line 9 (1 Transfer)**

        1.  **Walk**: From Gyeongbokgung Palace, walk about 5 minutes to **Gyeongbokgung Station (Line 3)**, Exit 5.
        2.  **Subway**: At Gyeongbokgung Station, take the **Line 3** train bound for **Ogeum** for 9 stops and get off at **Express Bus Terminal Station**. (Approx. 18 min)
        3.  **Transfer**: At Express Bus Terminal Station, transfer to the **Line 9 Express** train bound for **VHS Medical Center**.
        4.  **Subway**: Take the Line 9 Express train for 2 stops and get off at **Bongeunsa Station**. (Approx. 7 min)
        5.  **Walk**: Go out Exit 7 of Bongeunsa Station and follow the signs for about 2 minutes to the COEX Aquarium entrance.

        **Summary**
        * **Estimated Total Time**: Approx. 45 minutes (including walking and transfer time)
        * **Estimated Fare**: 1,500 KRW (with a transit card)

        **Alternative Route: Line 3 â†’ Line 2 (1 Transfer)**
        * You can also take Line 3 from Gyeongbokgung Station to Euljiro 3-ga Station, then transfer to Line 2 and get off at Samseong (World Trade Center) Station. This requires a bit more walking from the station to the aquarium but can be a good alternative if you find the transfer at Express Bus Terminal complex."""},
            {"role": "user",
             "content": "What's the fastest way to get from Hongik University Station to the main gate of Seoul National University? I don't mind taking a bus or subway."},
            {"role": "assistant", "content": f"""The fastest way from Hongik University Station to the main gate of Seoul National University involves taking both the subway and a bus.

        **Recommended Route: Line 2 â†’ Bus No. 5511**

        1.  **Subway**: At **Hongik Univ. Station (Line 2)**, take the Inner Circle Line train bound for **Sillim** for 8 stops and get off at **Seoul Nat'l Univ. Station**. (Approx. 16 min)
        2.  **Exit & Bus Stop**: Go out **Exit 3** of Seoul Nat'l Univ. Station and walk straight. The bus stop is right in front of you.
        3.  **Bus**: Take the **Green (Branch) Bus No. 5511** for 4 stops and get off at the **Seoul National University Main Gate** stop. (Approx. 10-15 min, depending on traffic)

        **Summary**
        * **Estimated Total Time**: Approx. 35-40 minutes (including walking and waiting time)
        * **Estimated Fare**: 1,500 KRW (with a transit card, including transfer discount)

        **Note**
        * During rush hour, Gwanak-ro (the road to the university) can be congested, which may increase the bus travel time slightly."""},
            {"role": "user", "content": text_input}
        ]
        try:
            print(f"\n--- [ì ‘ê·¼ ë°©ì‹ A] 1. ì›ë³¸ ì–¸ì–´({original_lang_name_en})ë¡œ ë‹µë³€ ìƒì„± ---")
            completion = self.gpt_model.chat.completions.create(model="gpt-4o", messages=messages,
                                                                max_tokens=1024,
                                                                temperature=0.7)
            original_language_answer = completion.choices[0].message.content.strip()
            print(f"  âœ… ìƒì„±ëœ ì›ë³¸ ì–¸ì–´ ë‹µë³€: {original_language_answer[:100]}...")

            print(f"\n--- [ì ‘ê·¼ ë°©ì‹ A] 2. ìƒì„±ëœ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­ ---")
            korean_answer = self.universal_translation_sync(original_language_answer, original_lang_name_en, "Korean")

            return {"answer_original_language": original_language_answer, "answer_korean": korean_answer}
        except Exception as e:
            print(f"âŒ GPT ë‹µë³€ ìƒì„± ë˜ëŠ” ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return default_answers

    def generate_speech_sync(self, text: str, lang_code: str) -> Optional[bytes]:
        if not self.kokoro_tts_pipeline:
            print("âš ï¸ Kokoro TTS íŒŒì´í”„ë¼ì¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ TTS ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None
        if not text or not text.strip():
            return None

        # Kokoroê°€ ì§€ì›í•˜ëŠ” ì–¸ì–´ ì½”ë“œì¸ì§€ í™•ì¸
        # ê°ì§€ëœ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'en', 'es', 'fr')ë¥¼ Kokoroì˜ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'a', 'e', 'f')ë¡œ ë§¤í•‘
        # pycountry.languages.get(alpha_2='en').alpha_1ì´ ìˆë‹¤ë©´ ê·¸ê±¸ ì‚¬ìš©. ì—†ìœ¼ë©´ ìˆ˜ë™ ë§¤í•‘.
        kokoro_lang_code_map = {
            'en': 'a', 'es': 'e', 'fr': 'f', 'hi': 'h', 'it': 'i', 'ja': 'j', 'pt': 'p', 'zh': 'z'
        }
        # pycountry alpha_1ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ì§ì ‘ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # ë˜í•œ 'b' (British English)ëŠ” 'a' (American English)ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
        kokoro_lang_char = kokoro_lang_code_map.get(lang_code.split('-')[0].lower())

        if kokoro_lang_char is None or kokoro_lang_char not in KOKORO_SUPPORTED_LANGS:
            print(f"âš ï¸ Kokoro TTSëŠ” '{lang_code}' ì–¸ì–´ì— ëŒ€í•œ ìŒì„± ìƒì„±ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì§€ì› ì–¸ì–´: {', '.join(KOKORO_SUPPORTED_LANGS)})")
            return None

        # í•´ë‹¹ ì–¸ì–´ì— ëŒ€í•œ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¦¬ì…‹ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        available_voice_presets = KOKORO_VOICE_PRESETS.get(kokoro_lang_char)

        if not available_voice_presets:
            print(f"âš ï¸ ì–¸ì–´ ì½”ë“œ '{kokoro_lang_char}'ì— ëŒ€í•œ Kokoro ìŒì„± í”„ë¦¬ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŒì„± ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None

        # ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¦¬ì…‹ ì¤‘ í•˜ë‚˜ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒ
        voice_preset = random.choice(available_voice_presets)

        print(
            f"\n--- ğŸ”Š Kokoro TTS ìƒì„± ì‹œì‘ (í…ìŠ¤íŠ¸ ì–¸ì–´: {lang_code}, Kokoro ì½”ë“œ: '{kokoro_lang_char}', ì‚¬ìš© í”„ë¦¬ì…‹: {voice_preset}) ---")
        try:
            # KPipeline ì¸ìŠ¤í„´ìŠ¤ì— í…ìŠ¤íŠ¸ì™€ voice í”„ë¦¬ì…‹ ì „ë‹¬
            # ê³µì‹ ë¬¸ì„œ ì˜ˆì‹œì—ì„œëŠ” generatorê°€ ì´í„°ë ˆì´í„°ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ, ì²« ë²ˆì§¸ ì˜¤ë””ì˜¤ ì²­í¬ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            generator = self.kokoro_tts_pipeline(text, voice=voice_preset)

            # ì²« ë²ˆì§¸ ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            first_chunk = next(generator, None)

            if first_chunk is None:
                print("âŒ Kokoro TTS ìƒì„±ê¸°ì—ì„œ ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return None

            audio_array = first_chunk[2]
            sampling_rate = 24000  # Kokoro ê³µì‹ ë¬¸ì„œ ì˜ˆì‹œì—ì„œ ì‚¬ìš©ëœ ìƒ˜í”Œë§ ë ˆì´íŠ¸

            # ë©”ëª¨ë¦¬ ë‚´ì—ì„œ WAV í¬ë§·ìœ¼ë¡œ ë³€í™˜
            buffer = io.BytesIO()
            sf.write(buffer, audio_array, sampling_rate, format='WAV')
            buffer.seek(0)
            audio_bytes = buffer.read()

            print("âœ… Kokoro TTS ìŒì„± ìƒì„± ì™„ë£Œ.")
            return audio_bytes

        except Exception as e:
            print(f"âŒ Kokoro TTS ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def transcription(self, audio_file: UploadFile, model_name: str) -> Dict[str, Any]:
        default_response = {
            "text": "", "language_code": "unknown", "language_name": "Unknown",
            "language_probability": 0.0, "original_query": "", "korean_query": "",
            "original_language_answer": "", "korean_answer": "",
            "korean_answer_audio_base64": None,  # í•œêµ­ì–´ ìŒì„± ë°˜í™˜ ì œê±°
            "original_language_answer_audio_base64": None
        }

        try:
            target_pipe = self.pipelines.get(model_name)
            if not target_pipe:
                error_message = f"ìš”ì²­í•œ ëª¨ë¸ '{model_name}'ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(self.pipelines.keys())}"
                return {**default_response, "error": "ModelNotFoundError", "message": error_message}

            audio_bytes = await audio_file.read()
            try:
                print(" attempting to read audio with soundfile...")
                audio_array, sampling_rate = sf.read(io.BytesIO(audio_bytes))
                print("âœ… Audio successfully read with soundfile.")
            except sf.LibsndfileError:
                print("âš ï¸ soundfile failed. Trying with pydub (likely m4a or mp3)...")
                try:
                    audio_file_like_object = io.BytesIO(audio_bytes)
                    audio_segment = AudioSegment.from_file(audio_file_like_object)
                    audio_segment = audio_segment.set_frame_rate(16000)
                    audio_segment = audio_segment.set_channels(1)
                    audio_array = np.array(audio_segment.get_array_of_samples())
                    if audio_array.dtype == np.int16:
                        audio_array = audio_array.astype(np.float32) / 32768.0
                    sampling_rate = audio_segment.frame_rate
                    print(f"âœ… Audio successfully processed with pydub. Sample rate: {sampling_rate}")
                except Exception as e_pydub:
                    return {**default_response, "error": "AudioProcessingError",
                            "message": f"ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ (pydub): {e_pydub}"}

            if audio_array.dtype != np.float32: audio_array = audio_array.astype(np.float32)
            if audio_array.ndim > 1: audio_array = np.mean(audio_array, axis=1)

            loaded_audio_sample = {"array": audio_array, "sampling_rate": sampling_rate}
            print(f"âœ… ì˜¤ë””ì˜¤ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {audio_file.filename}")

            print("\n--- ğŸ” ì–¸ì–´ ê°ì§€ ì‹œì‘ (`openai-whisper` ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©) ---")
            language_info = {"code": "unknown", "name": "Unknown", "probability": 0.0}
            if self.whisper_detection_model:
                try:
                    trimmed_audio = whisper.pad_or_trim(audio_array)
                    mel = whisper.log_mel_spectrogram(trimmed_audio).to(self.whisper_detection_model.device)
                    _, probs = self.whisper_detection_model.detect_language(mel)
                    original_lang_code = max(probs, key=probs.get)
                    lang_probability = probs[original_lang_code]
                    lang_name = pycountry.languages.get(alpha_2=original_lang_code).name if pycountry.languages.get(
                        alpha_2=original_lang_code) else original_lang_code
                    language_info.update({
                        'code': original_lang_code,
                        'name': lang_name,
                        'probability': round(lang_probability, 2)
                    })
                    print(f"âœ… ê°ì§€ëœ ì–¸ì–´ ì½”ë“œ: '{original_lang_code}' (í™•ë¥ : {lang_probability:.2f})")
                except Exception as e:
                    print(f"âš ï¸ `detect_language` ì–¸ì–´ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}. STTëŠ” ìë™ ê°ì§€ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            else:
                print("âš ï¸ (ì–¸ì–´ ê°ì§€ìš©) Whisper ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ì–¸ì–´ ê°ì§€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

            print(f"\n--- ğŸ—£ï¸ STT ë³€í™˜ ì‘ì—… ì‹œì‘ (ê°ì§€ëœ ì–¸ì–´: {language_info['code']}) ---")
            generate_kwargs = {"language": language_info['code']} if language_info['code'] != 'unknown' else {}
            result = await run_in_threadpool(target_pipe, loaded_audio_sample.copy(), generate_kwargs=generate_kwargs)
            transcribed_text = result.get("text", "").strip()
            print(f"âœ… STT ë³€í™˜ëœ í…ìŠ¤íŠ¸: {transcribed_text}")

            translated_question_korean = ""
            if transcribed_text and language_info['code'] != 'ko':
                translated_question_korean = await run_in_threadpool(self.universal_translation_sync, transcribed_text,
                                                                     language_info['name'], "Korean")
            elif language_info['code'] == 'ko':
                translated_question_korean = transcribed_text

            llm_answers = {"answer_original_language": "ë¯¸ì‹¤í–‰", "answer_korean": "ë¯¸ì‹¤í–‰"}
            if transcribed_text and language_info['code'] != 'unknown':
                llm_answers = await run_in_threadpool(self.generate_llm_response_sync, transcribed_text,
                                                      language_info['code'])

            korean_audio_base64 = None  # í•œêµ­ì–´ ìŒì„± ë°˜í™˜ì„ í•­ìƒ Noneìœ¼ë¡œ ì„¤ì •
            original_audio_base64 = None

            # ì›ë³¸ ì–¸ì–´ ë‹µë³€ ìŒì„± ìƒì„± (í•œêµ­ì–´ëŠ” ì œì™¸)
            original_answer_text = llm_answers.get("answer_original_language")
            if original_answer_text and original_answer_text not in ["ë¯¸ì‹¤í–‰", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"]:
                # í•œêµ­ì–´(ko)ê°€ ì•„ë‹ ë•Œë§Œ ìŒì„± ìƒì„± ì‹œë„
                if language_info['code'] != 'ko':
                    original_audio_bytes = await run_in_threadpool(self.generate_speech_sync, original_answer_text,
                                                                   language_info['code'])
                    if original_audio_bytes:
                        original_audio_base64 = base64.b64encode(original_audio_bytes).decode('utf-8')
                else:
                    print(f"âš ï¸ ì›ë³¸ ì–¸ì–´ê°€ í•œêµ­ì–´ì´ë¯€ë¡œ, í•œêµ­ì–´ ë‹µë³€ ìŒì„± ìƒì„±ì€ ê±´ë„ˆëœë‹ˆë‹¤.")

            final_response = {
                "original_query": transcribed_text,
                "language_code": language_info['code'],
                "language_name": language_info['name'],
                "language_probability": language_info['probability'],
                "korean_query": translated_question_korean,
                "original_language_answer": llm_answers.get("answer_original_language"),
                "korean_answer": llm_answers.get("answer_korean"),
                "korean_answer_audio_base64": korean_audio_base64,
                "original_language_answer_audio_base64": original_audio_base64
            }
            print(f"\nâœ… ìµœì¢… ì‘ë‹µ ì¤€ë¹„ ì™„ë£Œ (ìŒì„± í¬í•¨).")
            return final_response

        except Exception as e:
            import traceback
            traceback.print_exc()
            return {**default_response, "error": "TranscriptionProcessError", "message": f"ìŒì„± ì¸ì‹ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"}