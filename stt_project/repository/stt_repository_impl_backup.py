from abc import ABC, abstractmethod
import os, pycountry

import openai, json
import torch
from dotenv import load_dotenv
from fastapi import UploadFile
from starlette.concurrency import run_in_threadpool
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, Gemma3ForCausalLM, AutoTokenizer
from transformers import pipeline
from stt_project.repository.stt_repository import SttRepository  # ì´ ë¶€ë¶„ì€ ì‹¤ì œ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
import soundfile as sf
import numpy as np
import io
from mediapipe.tasks import python as mp_python
from mediapipe.tasks.python import text as mp_text
from typing import Optional, Dict, Any

class SttRepositoryImpl(SttRepository):
    __instance = None
    load_dotenv()

    def __new__(cls, *args, **kwargs):
        if cls.__instance is None:
            cls.__instance = super().__new__(cls)
        return cls.__instance

    @classmethod
    def getInstance(cls):
        if cls.__instance is None:
            cls.__instance = cls()
        return cls.__instance

    def __init__(self):
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            self.torch_dtype = torch.bfloat16
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            self.torch_dtype = torch.bfloat16
        else:
            self.device = torch.device("cpu")
            self.torch_dtype = torch.float16

        self.model = None
        self.processor = None
        self.common_pipe = None
        self.pipelines = {}
        self.translator_model = None  # Gemma ëª¨ë¸ (ê¸°ì¡´ translation_task_syncìš©)
        self.translator_tokenizer = None  # Gemma í† í¬ë‚˜ì´ì €
        self.gpt_model = None  # OpenAI ëª¨ë¸ (STT ê²°ê³¼ í•œêµ­ì–´ ë²ˆì—­ ë° LLM ë‹µë³€ ìƒì„±ìš©)

        self.get_model()
        print("--- SttRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì™„ë£Œ ---")

    def get_model(self):
        model_name = os.getenv("STTMODEL")
        trans_name = os.getenv("TRANSMODEL")  # Gemma ëª¨ë¸ëª…
        huggingface_token = os.getenv("HUGGINGFACE_TOKEN")

        model_names_str = os.getenv("STTMODELS", "openai/whisper-large-v3-turbo")  # ê¸°ë³¸ê°’ìœ¼ë¡œ large-v3-turbo ì„¤ì •
        model_names = [name.strip() for name in model_names_str.split(',')]

        for model_name in model_names:
            print(f"\n--- Whisper ëª¨ë¸ ({model_name}) ë¡œë“œ ì‹œì‘ ---")
            try:
                processor = AutoProcessor.from_pretrained(model_name)
                model = AutoModelForSpeechSeq2Seq.from_pretrained(
                    model_name, torch_dtype=self.torch_dtype, low_cpu_mem_usage=(self.device.type == "cpu"),
                    use_safetensors=True
                ).to(self.device)

                # í•´ë‹¹ ëª¨ë¸ì˜ íŒŒì´í”„ë¼ì¸ì„ ìƒì„±í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥
                self.pipelines[model_name] = pipeline(
                    "automatic-speech-recognition", model=model, tokenizer=processor.tokenizer,
                    feature_extractor=processor.feature_extractor, torch_dtype=self.torch_dtype,
                    device=self.device, return_timestamps=True,
                )
                print(f"âœ… Whisper ëª¨ë¸ ({model_name}) ë° ASR íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ.")
            except Exception as e:
                print(f"âŒ Whisper ëª¨ë¸ ({model_name}) ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

        # ... (MediaPipe, OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ëŠ” ê¸°ì¡´ê³¼ ë™ì¼) ...
        print("\n--- get_model_and_client ì™„ë£Œ ---")

        print("âœ… STT íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ")

        # OpenAI GPT ëª¨ë¸ ì´ˆê¸°í™”
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if openai_api_key:
            self.gpt_model = openai.OpenAI(api_key=openai_api_key)
            print("âœ… OpenAI GPT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")
        else:
            print("âš ï¸ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ì–´ OpenAI GPT í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.gpt_model = None

        self.mediapipe_language_detector = None
        try:
            language_detector_model_path = os.getenv("MEDIAPIPE_LANG_DETECTOR_MODEL_PATH", "models/detector.tflite")
            if not os.path.exists(language_detector_model_path):
                print(f"âš ï¸ ê²½ê³ : MediaPipe ì–¸ì–´ ê°ì§€ ëª¨ë¸ íŒŒì¼({language_detector_model_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                base_options = mp_python.BaseOptions(model_asset_path=language_detector_model_path)
                options = mp_text.LanguageDetectorOptions(base_options=base_options)
                self.mediapipe_language_detector = mp_text.LanguageDetector.create_from_options(options)
                print("âœ… MediaPipe Language Detector ì´ˆê¸°í™” ì™„ë£Œ.")
        except Exception as e:
            print(f"âŒ MediaPipe Language Detector ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        print("--- SttRepositoryImpl: get_model successfully ---")

    def translate_text_to_korean_with_gpt_sync(self, text_to_translate: str) -> str:
        """
        OpenAI GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
        """
        if not self.gpt_model:
            print("âš ï¸ OpenAI í´ë¼ì´ì–¸íŠ¸(gpt_model)ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ GPT í•œêµ­ì–´ ë²ˆì—­ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "GPT ë²ˆì—­ ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€ (í´ë¼ì´ì–¸íŠ¸ ë¯¸ì„¤ì •)"

        if not text_to_translate or not text_to_translate.strip():
            print("âš ï¸ ë²ˆì—­í•  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ GPT í•œêµ­ì–´ ë²ˆì—­ì„ ê±´ë„ˆ<0xEB><0x81><0x84>ë‹ˆë‹¤.")
            return ""  # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ë¹ˆ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜

        messages = [
            {"role": "system",
             "content": "You are a helpful assistant that translates text into Korean. Provide only the Korean translation, without any introductory phrases or explanations."},
            {"role": "user", "content": f"Translate the following text into Korean:\n\n\"{text_to_translate}\""}
        ]

        try:
            print(f"  OpenAI GPT API í˜¸ì¶œ (í•œêµ­ì–´ ë²ˆì—­) - ì…ë ¥ (ì• 100ì): {text_to_translate[:100]}...")
            completion = self.gpt_model.chat.completions.create(
                model="gpt-4o-mini",  # í•„ìš”ì‹œ ëª¨ë¸ ë³€ê²½ (e.g., "gpt-3.5-turbo")
                messages=messages,
                temperature=0.1,  # ë²ˆì—­ì€ ì¼ê´€ì„±ì´ ì¤‘ìš”í•˜ë¯€ë¡œ ë‚®ì€ ì˜¨ë„ë¡œ ì„¤ì •
                max_tokens=1024,  # ë²ˆì—­ ê²°ê³¼ì— ì¶©ë¶„í•œ í† í° í• ë‹¹ (ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´ ê³ ë ¤)
                top_p=1.0,
            )
            translated_text = completion.choices[0].message.content.strip()
            print(f"  GPT í•œêµ­ì–´ ë²ˆì—­ ê²°ê³¼ (ì²« 100ì): {translated_text[:100]}...")
            return translated_text
        except Exception as e:
            print(f"âŒ OpenAI GPT í•œêµ­ì–´ ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # import traceback; traceback.print_exc() # ë””ë²„ê¹…ìš©
            return f"GPT í•œêµ­ì–´ ë²ˆì—­ ì˜¤ë¥˜: {e}"

    def detect_language_with_mediapipe(self, text_to_detect: str) -> dict:
        default_result = {"code": "unknown", "name": "Unknown", "probability": 0.0}
        if not self.mediapipe_language_detector:
            print("âš ï¸ MediaPipe Language Detectorê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ ì–¸ì–´ ê°ì§€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return default_result
        if not text_to_detect or text_to_detect.strip() == "":
            print("âš ï¸ ì–¸ì–´ ê°ì§€ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ ê³µë°±ì…ë‹ˆë‹¤.")
            return default_result
        try:
            detection_result = self.mediapipe_language_detector.detect(text_to_detect)
            if detection_result.detections:
                best_detection = max(detection_result.detections, key=lambda d: d.probability)
                detected_lang_code = best_detection.language_code
                probability = round(best_detection.probability, 2)
                lang_name = "Unknown"
                code_to_lookup = detected_lang_code.split('-')[0].split('_')[0].lower()
                try:
                    lang_obj = None
                    if len(code_to_lookup) == 2:
                        lang_obj = pycountry.languages.get(alpha_2=code_to_lookup)
                    elif len(code_to_lookup) == 3:
                        lang_obj = pycountry.languages.get(alpha_3=code_to_lookup)

                    if lang_obj and hasattr(lang_obj, 'name'):
                        lang_name = lang_obj.name
                    # ... (ê¸°ì¡´ ìˆ˜ë™ ë§¤í•‘ ë¡œì§ ìœ ì§€) ...
                    elif detected_lang_code.lower() == 'zh-hant':
                        lang_name = 'Chinese (Traditional)'
                    elif detected_lang_code.lower() == 'zh-hans':
                        lang_name = 'Chinese (Simplified)'
                    else:
                        lang_name = detected_lang_code
                except Exception:
                    lang_name = detected_lang_code
                result = {"code": detected_lang_code, "name": lang_name, "probability": probability}
                print(f"  [MediaPipe & pycountry] ê°ì§€ëœ ì–¸ì–´ ì •ë³´: {result}")
                return result
            else:
                print("  [MediaPipe] ê°ì§€ëœ ì–¸ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return default_result
        except Exception as e:
            print(f"âŒ MediaPipe ì–¸ì–´ ê°ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return default_result

    def generate_llm_response_sync(self, text_input: str, original_language_code: str) -> dict:
        default_answers = {
            "answer_original_language": "ë‹µë³€ ìƒì„± ì‹¤íŒ¨ (ì›ë³¸ ì–¸ì–´)",
            "answer_korean": "ë‹µë³€ ìƒì„± ì‹¤íŒ¨ (í•œêµ­ì–´)"
        }
        if not self.gpt_model:
            print("âš ï¸ OpenAI í´ë¼ì´ì–¸íŠ¸(gpt_model)ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ GPT ë‹µë³€ ìƒì„±ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            default_answers["answer_original_language"] = "OpenAI API ì‚¬ìš© ë¶ˆê°€"
            default_answers["answer_korean"] = "OpenAI API ì‚¬ìš© ë¶ˆê°€"
            return default_answers
        # ... (ë‚˜ë¨¸ì§€ generate_llm_response_sync í•¨ìˆ˜ ë¡œì§ì€ ë™ì¼í•˜ê²Œ ìœ ì§€) ...
        if not text_input or text_input.strip() == "":
            default_answers["answer_original_language"] = "ì…ë ¥ í…ìŠ¤íŠ¸ ì—†ìŒ"
            default_answers["answer_korean"] = "ì…ë ¥ í…ìŠ¤íŠ¸ ì—†ìŒ"
            return default_answers

        original_lang_name_en = original_language_code
        try:
            lang_code_for_lookup = original_language_code.split('-')[0].split('_')[0].lower()
            lang_obj = None
            if len(lang_code_for_lookup) == 2:
                lang_obj = pycountry.languages.get(alpha_2=lang_code_for_lookup)
            elif len(lang_code_for_lookup) == 3:
                lang_obj = pycountry.languages.get(alpha_3=lang_code_for_lookup)
            if lang_obj and hasattr(lang_obj, 'name'): original_lang_name_en = lang_obj.name
        except:
            pass

        system_prompt = """You are an AI expert specializing in South Korean transportation. Your primary role is to provide the most optimal travel route from a starting point to a destination within South Korea.

        When providing the route, consider factors like travel time, cost, and convenience. Suggest various methods like subway, bus, train (KTX, SRT), or taxi where appropriate.

        Your response MUST be a single JSON object. This JSON object must contain exactly two keys: "answer_in_original_language" and "answer_in_korean".

        - The value for "answer_in_original_language" must be your answer, written in the **same language as the user's input**.
        - The value for "answer_in_korean" must be the direct Korean translation of that same answer.

        Both answers must convey the exact same meaning. Do not include any other text, explanations, or conversational remarks outside of the JSON object.

        **Example:**

        User's question (in English):
        `"What's the best way to get from Incheon International Airport to Myeongdong Station?"`

        Your response:
        ```json
        {
          "answer_in_original_language": "The most efficient way is to take the AREX (Airport Railroad Express) All-Stop Train from Incheon Int'l Airport T1 to Seoul Station. At Seoul Station, transfer to subway Line 4 and go 2 stops to Myeongdong Station. The total travel time is approximately 1 hour and 10 minutes.",
          "answer_in_korean": "ê°€ì¥ íš¨ìœ¨ì ì¸ ë°©ë²•ì€ ì¸ì²œêµ­ì œê³µí•­ T1ì—ì„œ ê³µí•­ì² ë„(AREX) ì¼ë°˜ì—´ì°¨ë¥¼ íƒ€ê³  ì„œìš¸ì—­ê¹Œì§€ ê°€ëŠ” ê²ƒì…ë‹ˆë‹¤. ì„œìš¸ì—­ì—ì„œ ì§€í•˜ì²  4í˜¸ì„ ìœ¼ë¡œ í™˜ìŠ¹í•˜ì—¬ 2ê°œ ì •ê±°ì¥ì„ ì´ë™í•˜ë©´ ëª…ë™ì—­ì— ë„ì°©í•©ë‹ˆë‹¤. ì´ ì†Œìš” ì‹œê°„ì€ ì•½ 1ì‹œê°„ 10ë¶„ì…ë‹ˆë‹¤."
        }
        ```"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"The user's text (in {original_lang_name_en}) is: \"{text_input}\""}
        ]
        try:
            print(
                f"  OpenAI API í˜¸ì¶œ (gpt-4o-mini, JSON ëª¨ë“œ) - ì›ë³¸ ì–¸ì–´: {original_lang_name_en}, ì…ë ¥ (ì• 100ì): {text_input[:100]}...")
            completion = self.gpt_model.chat.completions.create(
                model="gpt-4o-mini", messages=messages, response_format={"type": "json_object"},
                max_tokens=512, temperature=0.7, top_p=0.9
            )
            response_content = completion.choices[0].message.content
            print(f"  OpenAI API JSON ì‘ë‹µ (Raw): {response_content[:200]}...")
            parsed_response = json.loads(response_content)
            llm_answer_original = parsed_response.get("answer_in_original_language", "JSON íŒŒì‹± ì‹¤íŒ¨ (ì›ë³¸ ì–¸ì–´ ë‹µë³€)")
            llm_answer_korean = parsed_response.get("answer_in_korean", "JSON íŒŒì‹± ì‹¤íŒ¨ (í•œêµ­ì–´ ë‹µë³€)")
            return {
                "answer_original_language": llm_answer_original,
                "answer_korean": llm_answer_korean
            }
        except json.JSONDecodeError as e_json:
            print(
                f"GPT ì‘ë‹µ JSON íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e_json}. Raw response: {response_content if 'response_content' in locals() else 'N/A'}")
            default_answers["answer_original_language"] = f"GPT ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ (JSON íŒŒì‹± ì‹¤íŒ¨)"
            default_answers["answer_korean"] = f"GPT ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ (JSON íŒŒì‹± ì‹¤íŒ¨)"
            return default_answers
        except Exception as e:
            print(f"GPT ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ (JSON ëª¨ë“œ): {e}")
            default_answers["answer_original_language"] = f"GPT ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            default_answers["answer_korean"] = f"GPT ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            return default_answers

    async def transcription(self, audio_file: UploadFile, model_name: str) -> Dict[str, Any]:
        # --- ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡° ---
        default_response = {
            "text": "", "language_code": "unknown", "language_name": "Unknown",
            "language_probability": 0.0, "translated_text_korean": "",
            "llm_answer_original_language": "", "llm_answer_korean": ""
        }
        try:
            # --- 1. ìš”ì²­ëœ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì„ íƒ ---
            print(f"ìŒì„± ì¸ì‹ ìš”ì²­ - íŒŒì¼: {audio_file.filename}, ìš”ì²­ ëª¨ë¸: {model_name}")
            target_pipe = self.pipelines.get(model_name)
            if not target_pipe:
                available_models = list(self.pipelines.keys())
                error_message = f"ìš”ì²­í•œ ëª¨ë¸ '{model_name}'ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models}"
                print(f"âŒ {error_message}")
                return {**default_response, "error": "ModelNotFoundError", "message": error_message}

            # --- 2. ì˜¤ë””ì˜¤ íŒŒì¼ ì¤€ë¹„ ---
            audio_bytes = await audio_file.read()
            try:
                audio_array, sampling_rate = sf.read(io.BytesIO(audio_bytes))
            except sf.LibsndfileError as e_sf:
                return {**default_response, "error": "AudioProcessingError", "message": f"ì˜¤ë””ì˜¤ íŒŒì¼ ì˜¤ë¥˜: {e_sf}"}

            if audio_array.dtype != np.float32: audio_array = audio_array.astype(np.float32)
            if audio_array.ndim > 1 and audio_array.shape[1] > 1: audio_array = np.mean(audio_array, axis=1)

            loaded_audio_sample = {"array": audio_array, "sampling_rate": sampling_rate}
            print(f"âœ… ì˜¤ë””ì˜¤ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {audio_file.filename}")

            # --- âœ¨ ìˆ˜ì •ëœ ë¶€ë¶„: STTë¥¼ í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ëŠ” ë‹¨ìˆœí™”ëœ ë¡œì§ âœ¨ ---
            print("\n--- ğŸ—£ï¸ STT ë³€í™˜ ì‘ì—… ì‹œì‘ (ì˜¤ë””ì˜¤ ê¸°ë°˜ ìë™ ì–¸ì–´ ê°ì§€) ---")

            # ì–¸ì–´ ì˜µì…˜ ì—†ì´ íŒŒì´í”„ë¼ì¸ì„ í˜¸ì¶œí•˜ì—¬ Whisperê°€ ì˜¤ë””ì˜¤ì—ì„œ ì§ì ‘ ì–¸ì–´ë¥¼ ê°ì§€í•˜ë„ë¡ í•¨
            result = await run_in_threadpool(target_pipe, loaded_audio_sample.copy())
            transcribed_text = result.get("text", "").strip()
            print(f"âœ… STT ë³€í™˜ëœ í…ìŠ¤íŠ¸: {transcribed_text}")

            # STT ê²°ê³¼ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–´ë–¤ ì–¸ì–´ì˜€ëŠ”ì§€ í™•ì¸
            language_info = {"code": "unknown", "name": "Unknown", "probability": 0.0}
            if transcribed_text:
                print("\n--- ğŸ” MediaPipe ì–¸ì–´ í™•ì¸ ì‹œì‘ ---")
                language_info = await run_in_threadpool(self.detect_language_with_mediapipe, transcribed_text)
            else:
                print("  âš ï¸ STT í…ìŠ¤íŠ¸ê°€ ì—†ì–´ ì–¸ì–´ í™•ì¸ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

            original_lang_code = language_info.get('code', 'unknown')
            # --- âœ¨ ë¡œì§ ìˆ˜ì • ë âœ¨ ---

            # --- 4. GPT í•œêµ­ì–´ ë²ˆì—­ ---
            translated_text_korean_by_gpt = ""
            if transcribed_text and original_lang_code != 'ko':
                if self.gpt_model:
                    print(f"\nğŸŒ STT í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­ ì‹œë„ (GPT ì‚¬ìš©)...")
                    translated_text_korean_by_gpt = await run_in_threadpool(
                        self.translate_text_to_korean_with_gpt_sync,
                        transcribed_text
                    )
                    print(f"  ğŸ‡°ğŸ‡· GPT ë²ˆì—­ ê²°ê³¼: {translated_text_korean_by_gpt[:100]}...")
                else:
                    translated_text_korean_by_gpt = "GPT ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€"
            elif original_lang_code == 'ko':
                translated_text_korean_by_gpt = transcribed_text
            else:
                print("  âš ï¸ STT í…ìŠ¤íŠ¸ê°€ ì—†ì–´ GPT í•œêµ­ì–´ ë²ˆì—­ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

            # --- 5. LLM ë‹µë³€ ìƒì„± ---
            llm_answers = {"answer_original_language": "LLM ë‹µë³€ ë¯¸ìƒì„±", "answer_korean": "LLM ë‹µë³€ ë¯¸ìƒì„±"}
            if transcribed_text:
                if self.gpt_model:
                    if original_lang_code != 'unknown':
                        print(f"\nğŸ’¬ STT í…ìŠ¤íŠ¸ì— ëŒ€í•œ LLM ë‹µë³€ ìƒì„± ì‹œë„ (ì›ë³¸ ì–¸ì–´: {original_lang_code})...")
                        llm_answers = await run_in_threadpool(self.generate_llm_response_sync, transcribed_text,
                                                              original_lang_code)
                        print(f"  ğŸ’¡ LLM ìƒì„± ë‹µë³€ (ì›ë³¸): {llm_answers.get('answer_original_language', '')[:100]}...")
                        print(f"  ğŸ’¡ LLM ìƒì„± ë‹µë³€ (í•œêµ­ì–´): {llm_answers.get('answer_korean', '')[:100]}...")
                    else:
                        llm_answers["answer_original_language"] = llm_answers["answer_korean"] = "ì›ë³¸ ì–¸ì–´ ë¶ˆëª…í™•"
                else:
                    llm_answers["answer_original_language"] = llm_answers["answer_korean"] = "GPT ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€"
            else:
                llm_answers["answer_original_language"] = llm_answers["answer_korean"] = "STT í…ìŠ¤íŠ¸ ì—†ìŒ"

            # --- 6. ìµœì¢… ì‘ë‹µ ë°˜í™˜ ---
            final_response = {
                "text": transcribed_text,
                "language_code": original_lang_code,
                "language_name": language_info.get('name', 'Unknown'),
                "language_probability": language_info.get('probability', 0.0),
                "translated_text_korean": translated_text_korean_by_gpt,
                "llm_answer_original_language": llm_answers.get("answer_original_language"),
                "llm_answer_korean": llm_answers.get("answer_korean")
            }
            print(f"\nâœ… ìµœì¢… ì‘ë‹µ ì¤€ë¹„ ì™„ë£Œ.")
            return final_response

        except Exception as e:
            import traceback
            print(f"âŒ ìŒì„± ì¸ì‹ ë° ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e} - íŒŒì¼: {audio_file.filename}")
            traceback.print_exc()
            return {**default_response,
                    "error": "TranscriptionProcessError",
                    "message": f"ìŒì„± ì¸ì‹ ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {audio_file.filename}. ({e})"}