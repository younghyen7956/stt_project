import asyncio
import os, pycountry
from pydub import AudioSegment
import openai, json
import torch
from dotenv import load_dotenv
from fastapi import UploadFile
from starlette.concurrency import run_in_threadpool
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor
from transformers import pipeline
import whisper
import soundfile as sf
import numpy as np
import io
from typing import Optional, Dict, Any
import ssl

ssl._create_default_https_context = ssl._create_unverified_context
from stt_project.repository.stt_repository import SttRepository

class SttRepositoryImpl(SttRepository):
    __instance = None
    load_dotenv()

    def __new__(cls, *args, **kwargs):
        if cls.__instance is None:
            cls.__instance = super().__new__(cls)
        return cls.__instance

    @classmethod
    def getInstance(cls):
        if cls.__instance is None:
            cls.__instance = cls()
        return cls.__instance

    def __init__(self):
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            self.torch_dtype = torch.bfloat16
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            self.torch_dtype = torch.bfloat16
        else:
            self.device = torch.device("cpu")
            self.torch_dtype = torch.float16

        self.pipelines = {}
        self.gpt_model = None
        self.whisper_detection_model = None
        self.get_model()
        print("--- SttRepositoryImpl: __init__ ìµœì´ˆ ì´ˆê¸°í™” ì™„ë£Œ ---")

    def get_model(self):
        model_names_str = os.getenv("STTMODELS", "openai/whisper-large-v3-turbo")
        model_names = [name.strip() for name in model_names_str.split(',')]

        for model_name in model_names:
            print(f"\n--- Whisper ëª¨ë¸ ({model_name}) ë¡œë“œ ì‹œì‘ ---")
            try:
                processor = AutoProcessor.from_pretrained(model_name)
                model = AutoModelForSpeechSeq2Seq.from_pretrained(
                    model_name, torch_dtype=self.torch_dtype, low_cpu_mem_usage=(self.device.type == "cpu"),
                    use_safetensors=True
                ).to(self.device)

                self.pipelines[model_name] = pipeline(
                    "automatic-speech-recognition", model=model, tokenizer=processor.tokenizer,
                    feature_extractor=processor.feature_extractor, torch_dtype=self.torch_dtype,
                    device=self.device, return_timestamps=True,
                )
                print(f"âœ… Whisper ëª¨ë¸ ({model_name}) ë° ASR íŒŒì´í”„ë¼ì¸ ìƒì„± ì™„ë£Œ.")
            except Exception as e:
                print(f"âŒ Whisper ëª¨ë¸ ({model_name}) ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

        openai_api_key = os.getenv("OPENAI_API_KEY")
        if openai_api_key:
            self.gpt_model = openai.OpenAI(api_key=openai_api_key)
            print("âœ… OpenAI GPT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")
        else:
            print("âš ï¸ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ì–´ OpenAI GPT í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.gpt_model = None

        print("\n--- (ì–¸ì–´ ê°ì§€ìš©) Whisper ëª¨ë¸ ë¡œë“œ ì‹œì‘ ---")
        try:
            self.whisper_detection_model = whisper.load_model("base", device="cpu")
            print("âœ… (ì–¸ì–´ ê°ì§€ìš©) Whisper ëª¨ë¸ (base) ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            print(f"âŒ (ì–¸ì–´ ê°ì§€ìš©) Whisper ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

        print("--- SttRepositoryImpl: get_model successfully ---")

    def universal_translation_sync(self, text_to_translate: str, source_lang: str, target_lang: str) -> str:
        if not self.gpt_model: return "GPT ë²ˆì—­ ì„œë¹„ìŠ¤ ì‚¬ìš© ë¶ˆê°€"
        if not text_to_translate or not text_to_translate.strip(): return ""

        # Note: ì´ì „ ëŒ€í™”ì—ì„œ ì œì•ˆë“œë¦° ê³ í’ˆì§ˆ ë²ˆì—­ í”„ë¡¬í”„íŠ¸ ëŒ€ì‹ , í™•ì •í•´ì£¼ì‹  ë²„ì „ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        system_content = f"""You are a professional translator.
        Your task is to accurately translate the given text from {source_lang} to {target_lang}.
        Preserve the original formatting and meaning.
        Provide only the translated text, without any introductory phrases."""

        user_prompt = f"Translate the following text from {source_lang} to {target_lang}:\n\n{text_to_translate}"

        print(f"  (GPT ë²ˆì—­ ìš”ì²­: {source_lang} -> {target_lang} using gpt-4o-mini)")

        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_prompt}
        ]
        try:
            # Note: ì´ì „ ëŒ€í™”ì—ì„œ ì œì•ˆë“œë¦° gpt-4o ëŒ€ì‹ , í™•ì •í•´ì£¼ì‹  gpt-4o-mini ëª¨ë¸ì„ ìœ ì§€í•©ë‹ˆë‹¤.
            completion = self.gpt_model.chat.completions.create(model="gpt-4o-mini", messages=messages, temperature=0.0,
                                                                max_tokens=1024)
            return completion.choices[0].message.content.strip()
        except Exception as e:
            print(f"âŒ OpenAI GPT ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"GPT ë²ˆì—­ ì˜¤ë¥˜: {e}"

    def generate_llm_response_sync(self, text_input: str, original_language_code: str) -> dict:
        default_answers = {"answer_original_language": "ë‹µë³€ ìƒì„± ì‹¤íŒ¨", "answer_korean": "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"}
        if not self.gpt_model:
            default_answers["answer_original_language"] = "OpenAI API ì‚¬ìš© ë¶ˆê°€"
            default_answers["answer_korean"] = "OpenAI API ì‚¬ìš© ë¶ˆê°€"
            return default_answers
        if not text_input or text_input.strip() == "":
            default_answers["answer_original_language"] = "ì…ë ¥ í…ìŠ¤íŠ¸ ì—†ìŒ"
            default_answers["answer_korean"] = "ì…ë ¥ í…ìŠ¤íŠ¸ ì—†ìŒ"
            return default_answers

        original_lang_name_en = original_language_code
        try:
            lang_obj = pycountry.languages.get(alpha_2=original_language_code.split('-')[0].lower())
            if lang_obj: original_lang_name_en = lang_obj.name
        except:
            pass

        system_prompt = f"""You are an AI expert specializing in South Korean public transportation. 
        Your primary role is to provide the most optimal travel route from a starting point to a destination within South Korea, in {original_lang_name_en}.

        Always follow these steps to structure your response:
        1.  Propose one "Recommended Route" that is the most efficient, considering a balance of travel time, number of transfers, and cost.
        2.  Describe each step of the Recommended Route in detail (e.g., walking, subway line, bus number, direction, travel time, getting off station, exit number).
        3.  You can also suggest one or two "Alternative Routes" that might be worth considering.
        4.  Conclude with a summary of the "Estimated Total Time" and "Estimated Fare".
        5.  Provide only the helpful answer, without any introductory phrases like "Of course, here is the route...".
        """

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": "How do I get from Gyeongbokgung Palace to the COEX Aquarium by subway?"},
            {"role": "assistant", "content": f"""Here is the most efficient subway route from Gyeongbokgung Palace to the COEX Aquarium.

        **Recommended Route: Line 3 â†’ Line 9 (1 Transfer)**

        1.  **Walk**: From Gyeongbokgung Palace, walk about 5 minutes to **Gyeongbokgung Station (Line 3)**, Exit 5.
        2.  **Subway**: At Gyeongbokgung Station, take the **Line 3** train bound for **Ogeum** for 9 stops and get off at **Express Bus Terminal Station**. (Approx. 18 min)
        3.  **Transfer**: At Express Bus Terminal Station, transfer to the **Line 9 Express** train bound for **VHS Medical Center**.
        4.  **Subway**: Take the Line 9 Express train for 2 stops and get off at **Bongeunsa Station**. (Approx. 7 min)
        5.  **Walk**: Go out Exit 7 of Bongeunsa Station and follow the signs for about 2 minutes to the COEX Aquarium entrance.

        **Summary**
        * **Estimated Total Time**: Approx. 45 minutes (including walking and transfer time)
        * **Estimated Fare**: 1,500 KRW (with a transit card)

        **Alternative Route: Line 3 â†’ Line 2 (1 Transfer)**
        * You can also take Line 3 from Gyeongbokgung Station to Euljiro 3-ga Station, then transfer to Line 2 and get off at Samseong (World Trade Center) Station. This requires a bit more walking from the station to the aquarium but can be a good alternative if you find the transfer at Express Bus Terminal complex."""},
            {"role": "user",
             "content": "What's the fastest way to get from Hongik University Station to the main gate of Seoul National University? I don't mind taking a bus or subway."},
            {"role": "assistant", "content": f"""The fastest way from Hongik University Station to the main gate of Seoul National University involves taking both the subway and a bus.

        **Recommended Route: Line 2 â†’ Bus No. 5511**

        1.  **Subway**: At **Hongik Univ. Station (Line 2)**, take the Inner Circle Line train bound for **Sillim** for 8 stops and get off at **Seoul Nat'l Univ. Station**. (Approx. 16 min)
        2.  **Exit & Bus Stop**: Go out **Exit 3** of Seoul Nat'l Univ. Station and walk straight. The bus stop is right in front of you.
        3.  **Bus**: Take the **Green (Branch) Bus No. 5511** for 4 stops and get off at the **Seoul National University Main Gate** stop. (Approx. 10-15 min, depending on traffic)

        **Summary**
        * **Estimated Total Time**: Approx. 35-40 minutes (including walking and waiting time)
        * **Estimated Fare**: 1,500 KRW (with a transit card, including transfer discount)

        **Note**
        * During rush hour, Gwanak-ro (the road to the university) can be congested, which may increase the bus travel time slightly."""},
            {"role": "user", "content": text_input}
        ]
        try:
            print(f"\n--- [ì ‘ê·¼ ë°©ì‹ A] 1. ì›ë³¸ ì–¸ì–´({original_lang_name_en})ë¡œ ë‹µë³€ ìƒì„± ---")
            completion = self.gpt_model.chat.completions.create(model="gpt-4o", messages=messages,
                                                                max_tokens=1024,
                                                                temperature=0.7)
            original_language_answer = completion.choices[0].message.content.strip()
            print(f"  âœ… ìƒì„±ëœ ì›ë³¸ ì–¸ì–´ ë‹µë³€: {original_language_answer[:100]}...")

            print(f"\n--- [ì ‘ê·¼ ë°©ì‹ A] 2. ìƒì„±ëœ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­ ---")
            korean_answer = self.universal_translation_sync(original_language_answer, original_lang_name_en, "Korean")

            return {"answer_original_language": original_language_answer, "answer_korean": korean_answer}
        except Exception as e:
            print(f"âŒ GPT ë‹µë³€ ìƒì„± ë˜ëŠ” ë²ˆì—­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return default_answers

    async def transcription(self, audio_file: UploadFile, model_name: str) -> Dict[str, Any]:
        # --- â–¼â–¼â–¼ ìµœì¢… ì¶œë ¥ í˜•ì‹ì— ë§ê²Œ default_response ìˆ˜ì • â–¼â–¼â–¼ ---
        default_response = {
            "text": "", "language_code": "unknown", "language_name": "Unknown",
            "language_probability": 0.0, "original_query": "", "korean_query": "",
            "original_language_answer": "", "korean_answer": ""
        }

        try:
            target_pipe = self.pipelines.get(model_name)
            if not target_pipe:
                error_message = f"ìš”ì²­í•œ ëª¨ë¸ '{model_name}'ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(self.pipelines.keys())}"
                return {**default_response, "error": "ModelNotFoundError", "message": error_message}

            audio_bytes = await audio_file.read()
            try:
                print(" attempting to read audio with soundfile...")
                audio_array, sampling_rate = sf.read(io.BytesIO(audio_bytes))
                print("âœ… Audio successfully read with soundfile.")
            except sf.LibsndfileError:
                print("âš ï¸ soundfile failed. Trying with pydub (likely m4a or mp3)...")
                try:
                    audio_file_like_object = io.BytesIO(audio_bytes)
                    audio_segment = AudioSegment.from_file(audio_file_like_object)
                    audio_segment = audio_segment.set_frame_rate(16000)
                    audio_segment = audio_segment.set_channels(1)
                    audio_array = np.array(audio_segment.get_array_of_samples())
                    if audio_array.dtype == np.int16:
                        audio_array = audio_array.astype(np.float32) / 32768.0
                    sampling_rate = audio_segment.frame_rate
                    print(f"âœ… Audio successfully processed with pydub. Sample rate: {sampling_rate}")
                except Exception as e_pydub:
                    return {**default_response, "error": "AudioProcessingError",
                            "message": f"ì˜¤ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ (pydub): {e_pydub}"}

            if audio_array.dtype != np.float32: audio_array = audio_array.astype(np.float32)
            if audio_array.ndim > 1: audio_array = np.mean(audio_array, axis=1)

            loaded_audio_sample = {"array": audio_array, "sampling_rate": sampling_rate}
            print(f"âœ… ì˜¤ë””ì˜¤ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {audio_file.filename}")

            print("\n--- ğŸ” ì–¸ì–´ ê°ì§€ ì‹œì‘ (`openai-whisper` ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©) ---")
            language_info = {"code": "unknown", "name": "Unknown", "probability": 0.0}
            if self.whisper_detection_model:
                try:
                    trimmed_audio = whisper.pad_or_trim(audio_array)
                    mel = whisper.log_mel_spectrogram(trimmed_audio).to(self.whisper_detection_model.device)
                    _, probs = self.whisper_detection_model.detect_language(mel)
                    original_lang_code = max(probs, key=probs.get)
                    lang_probability = probs[original_lang_code]
                    lang_name = pycountry.languages.get(alpha_2=original_lang_code).name if pycountry.languages.get(
                        alpha_2=original_lang_code) else original_lang_code
                    language_info.update({
                        'code': original_lang_code,
                        'name': lang_name,
                        'probability': round(lang_probability, 2)
                    })
                    print(f"âœ… ê°ì§€ëœ ì–¸ì–´ ì½”ë“œ: '{original_lang_code}' (í™•ë¥ : {lang_probability:.2f})")
                except Exception as e:
                    print(f"âš ï¸ `detect_language` ì–¸ì–´ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}. STTëŠ” ìë™ ê°ì§€ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            else:
                print("âš ï¸ (ì–¸ì–´ ê°ì§€ìš©) Whisper ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ì–¸ì–´ ê°ì§€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

            print(f"\n--- ğŸ—£ï¸ STT ë³€í™˜ ì‘ì—… ì‹œì‘ (ê°ì§€ëœ ì–¸ì–´: {language_info['code']}) ---")
            generate_kwargs = {"language": language_info['code']} if language_info['code'] != 'unknown' else {}
            result = await run_in_threadpool(target_pipe, loaded_audio_sample.copy(), generate_kwargs=generate_kwargs)
            transcribed_text = result.get("text", "").strip()
            print(f"âœ… STT ë³€í™˜ëœ í…ìŠ¤íŠ¸: {transcribed_text}")

            # STT ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì´í›„ ì‘ì—…ì„ ì§„í–‰í•˜ì§€ ì•ŠìŒ
            if not transcribed_text:
                return {**default_response, "original_query": "No speech detected."}

            # --- â–¼â–¼â–¼ 2. ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (í•µì‹¬ ë³€ê²½ì‚¬í•­) â–¼â–¼â–¼ ---
            print("\n--- ğŸš€ ë²ˆì—­ ë° LLM ë‹µë³€ ìƒì„± ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ---")

            # ì‘ì—… 1: ì›ë³¸ ì§ˆë¬¸ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ëŠ” íƒœìŠ¤í¬ (GPT ì‚¬ìš©)
            korean_query_task = None
            if language_info['code'] != 'ko':
                print("  - (Task A) 'ì›ë³¸ ì¿¼ë¦¬ -> í•œêµ­ì–´' ë²ˆì—­ íƒœìŠ¤í¬ ìƒì„±")
                korean_query_task = run_in_threadpool(
                    self.universal_translation_sync,
                    transcribed_text,
                    language_info['name'],  # GPT í”„ë¡¬í”„íŠ¸ì—ëŠ” 'Japanese' ê°™ì€ ì´ë¦„ì´ ë” íš¨ê³¼ì 
                    "Korean"
                )
            else:
                async def pass_through():
                    return transcribed_text

                korean_query_task = pass_through()
                print("  - (Task A) ì›ë³¸ ì¿¼ë¦¬ê°€ í•œêµ­ì–´ì´ë¯€ë¡œ ë²ˆì—­ ì—†ìŒ")

            # ì‘ì—… 2: LLM ë‹µë³€ì„ ìƒì„±í•˜ëŠ” íƒœìŠ¤í¬ (ë‚´ë¶€ì ìœ¼ë¡œ GPT ë²ˆì—­ í¬í•¨)
            print("  - (Task B) 'LLM ë‹µë³€ ìƒì„±' íƒœìŠ¤í¬ ìƒì„±")
            llm_answers_task = run_in_threadpool(
                self.generate_llm_response_sync,
                transcribed_text,
                language_info['code']
            )

            # asyncio.gatherë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ê°œì˜ ë¬´ê±°ìš´ ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¼
            print("  - [asyncio.gather] Task Aì™€ Task B ë™ì‹œ ì‹¤í–‰")
            task_results = await asyncio.gather(
                korean_query_task,
                llm_answers_task
            )
            print("  - âœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ")

            # ê²°ê³¼ ì •ë¦¬
            translated_question_korean = task_results[0]
            llm_answers = task_results[1]

            # --- â–¼â–¼â–¼ ìš”ì²­í•˜ì‹  ìµœì¢… í˜•íƒœë¡œ ì‘ë‹µ êµ¬ì„± â–¼â–¼â–¼ ---
            final_response = {
                "original_query": transcribed_text,
                "language_code": language_info['code'],
                "language_name": language_info['name'],
                "language_probability": language_info['probability'],
                "korean_query": translated_question_korean,
                "original_language_answer": llm_answers['answer_original_language'],
                "korean_answer": llm_answers['answer_korean']
            }
            print(f"\nâœ… ìµœì¢… ì‘ë‹µ ì¤€ë¹„ ì™„ë£Œ.")
            return final_response

        except Exception as e:
            import traceback
            traceback.print_exc()
            return {**default_response, "error": "TranscriptionProcessError", "message": f"ìŒì„± ì¸ì‹ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"}